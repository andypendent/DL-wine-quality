{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497, 12)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "path = '/Users/Andy/Documents/_BeCode/becode_projects/DL-wine-quality/Wine Quality/wine.csv'\n",
    "df = pd.read_csv(path)\n",
    "df = df.drop(['index'],axis=1)\n",
    "df.head()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature and target Data\n",
    "# Input 'X' is all but the last column\n",
    "# Output 'y' is the last column = 'quality'\n",
    "\n",
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497, 11)\n"
     ]
    }
   ],
   "source": [
    "X.head()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497,)\n"
     ]
    }
   ],
   "source": [
    "y.head()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5197, 11)\n",
      "(1300, 11)\n",
      "(5197,)\n",
      "(1300,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test\n",
    "# => Because thereâ€™s a class imbalance,\n",
    "# we want to have equal distribution of all output classes in our train and test sets.\n",
    "# To do that, we use the 'stratify' option in function 'train_test_split()'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=69)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: Counter({6: 2269, 5: 1710, 7: 863, 4: 173, 8: 154, 3: 24, 9: 4})\n"
     ]
    }
   ],
   "source": [
    "# nbr of samples per class in y_train\n",
    "print(f'y_train: {Counter(y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_jobs': None, 'n_neighbors': 3, 'random_state': 42, 'sampling_strategy': 'not majority'}\n"
     ]
    }
   ],
   "source": [
    "# resample train set with oversampler ADASYN\n",
    "\n",
    "sampling_strategy = 'not majority' # resample all classes but the majority class;\n",
    "n_neighbors = 3\n",
    "res = ADASYN(sampling_strategy=sampling_strategy, n_neighbors=n_neighbors, random_state=42)\n",
    "print(res.get_params())\n",
    "\n",
    "X_train_res, y_train_res = res.fit_resample(np.array(X_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_res: Counter({7: 2452, 8: 2314, 6: 2269, 9: 2268, 3: 2264, 4: 2223, 5: 2046})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15836,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after resampling: nbr of samples per class in y_train_res\n",
    "print(f'y_train_res: {Counter(y_train_res)}')\n",
    "y_train_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'one hot' encode y_train_res, y_test with 'pd.get_dummies'\n",
    "y_train_res = pd.get_dummies(y_train_res)\n",
    "y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       3  4  5  6  7  8  9\n",
       " 0      0  0  0  1  0  0  0\n",
       " 1      0  0  1  0  0  0  0\n",
       " 2      0  0  0  0  1  0  0\n",
       " 3      0  0  0  0  0  1  0\n",
       " 4      0  0  0  1  0  0  0\n",
       " ...   .. .. .. .. .. .. ..\n",
       " 15831  0  0  0  0  0  0  1\n",
       " 15832  0  0  0  0  0  0  1\n",
       " 15833  0  0  0  0  0  0  1\n",
       " 15834  0  0  0  0  0  0  1\n",
       " 15835  0  0  0  0  0  0  1\n",
       " \n",
       " [15836 rows x 7 columns],\n",
       "       3  4  5  6  7  8  9\n",
       " 475   0  0  1  0  0  0  0\n",
       " 4459  0  0  0  1  0  0  0\n",
       " 2245  0  1  0  0  0  0  0\n",
       " 5487  0  0  0  1  0  0  0\n",
       " 5410  0  0  1  0  0  0  0\n",
       " ...  .. .. .. .. .. .. ..\n",
       " 2399  0  0  0  1  0  0  0\n",
       " 2636  0  0  1  0  0  0  0\n",
       " 5652  0  0  0  0  1  0  0\n",
       " 2022  0  0  0  1  0  0  0\n",
       " 4947  0  0  0  1  0  0  0\n",
       " \n",
       " [1300 rows x 7 columns])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_res, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_train, y_train and their test counterparts into pytorch tensors using torch.tensor()\n",
    "# => before convert to np.array()\n",
    "# make sure to convert them to float\n",
    "\n",
    "X_train_res = torch.tensor(np.array(X_train_res), dtype=torch.float64)\n",
    "y_train_res = torch.tensor(np.array(y_train_res), dtype=torch.float64)\n",
    "X_test = torch.tensor(np.array(X_test), dtype=torch.float64)\n",
    "y_test = torch.tensor(np.array(y_test), dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.6000e+00, 3.2000e-01, 3.6000e-01, 1.6000e+00, 4.0000e-02, 3.2000e+01,\n",
       "         1.5500e+02, 9.9300e-01, 3.2300e+00, 5.2000e-01, 1.1300e+01],\n",
       "        [7.0000e+00, 4.2000e-01, 1.9000e-01, 2.3000e+00, 7.1000e-02, 1.8000e+01,\n",
       "         3.6000e+01, 9.9476e-01, 3.3900e+00, 5.6000e-01, 1.0900e+01],\n",
       "        [9.8000e+00, 5.0000e-01, 3.4000e-01, 2.3000e+00, 9.4000e-02, 1.0000e+01,\n",
       "         4.5000e+01, 9.9864e-01, 3.2400e+00, 6.0000e-01, 9.7000e+00],\n",
       "        [5.2000e+00, 1.5500e-01, 3.3000e-01, 1.6000e+00, 2.8000e-02, 1.3000e+01,\n",
       "         5.9000e+01, 9.8975e-01, 3.3000e+00, 8.4000e-01, 1.1900e+01],\n",
       "        [6.5000e+00, 3.1000e-01, 6.1000e-01, 1.3000e+01, 5.3000e-02, 3.1000e+01,\n",
       "         1.2300e+02, 9.9708e-01, 3.0900e+00, 5.0000e-01, 9.3000e+00]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_res[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_res[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define NN params\n",
    "nb_hidden_neurons = 438\n",
    "\n",
    "nb_features = X_train.shape[1]\n",
    "nb_classes = len(pd.unique(df['quality']))\n",
    "nb_features, nb_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN architecture\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_features):\n",
    "        \"\"\"Here we define the layers\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        # class torch.nn.Linear(in_features: int, out_features: int, bias: bool = True = default)\n",
    "        \n",
    "        self.layer_1 = nn.Linear(nb_features, nb_hidden_neurons)  \n",
    "        self.layer_2 = nn.Linear(nb_hidden_neurons, nb_hidden_neurons)\n",
    "        self.layer_3 = nn.Linear(nb_hidden_neurons, nb_classes)\n",
    "           \n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"Here we combine the layers\n",
    "        \"\"\"\n",
    "        \n",
    "        activation_function = nn.ReLU()\n",
    "        last_layer_activation = nn.Softmax(dim=1)\n",
    "        \n",
    "        output_first_layer = activation_function(self.layer_1(x)) \n",
    "        output_second_layer = activation_function(self.layer_2(output_first_layer))\n",
    "        prediction = last_layer_activation(self.layer_3(output_second_layer))\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (layer_1): Linear(in_features=11, out_features=438, bias=True)\n",
       "  (layer_2): Linear(in_features=438, out_features=438, bias=True)\n",
       "  (layer_3): Linear(in_features=438, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "my_nn = Network(nb_features)\n",
    "my_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MSELoss(),\n",
       " 0.001,\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     lr: 0.001\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select your criterion, your learning rate and your optimizer.\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.001 \n",
    "optimizer = torch.optim.Adam(my_nn.parameters(), learning_rate)\n",
    "\n",
    "criterion, learning_rate, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train NN\n",
    "\n",
    "def training(batch_size : int, nb_steps_loss_sum : int):\n",
    "    \"\"\" Train the neural network, feeding it `batch_size` at a time\n",
    "    and saving statistics every `nb_steps_loss_sum` steps.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    - batch_size [int] : the number of input samples at each training step (called a batch)\n",
    "    - nb_steps_loss_sum [int] : the number of batches before saving the loss for plotting\n",
    "    \n",
    "    Returns:\n",
    "    - loss_list : [List[double]] : value of the loss every `nb_steps_loss_sum` steps\n",
    "    \"\"\"\n",
    "\n",
    "    loss_list = []\n",
    "    running_loss = 0\n",
    "    batch_nb = 0\n",
    "\n",
    "    for epoch in range(0,10): # Number of times to iterate through the complete dataset\n",
    "        for idx in range(0, X_train_res.shape[0], batch_size):\n",
    "            \n",
    "            # Get input and output\n",
    "            input_batch = X_train_res[idx:idx + batch_size]\n",
    "            target = y_train_res[idx:idx + batch_size]\n",
    "            \n",
    "            # TO COMPLETE:\n",
    "            # - zero gradient buffers\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # - compute the forward pass\n",
    "            y_pred = my_nn(input_batch.float())\n",
    "            \n",
    "            # - compute the loss\n",
    "            loss = criterion(y_pred, target.float())\n",
    "            \n",
    "            # - backpropagate\n",
    "            loss.backward()\n",
    "            \n",
    "            # - do a step => update gradients\n",
    "            optimizer.step()\n",
    "          \n",
    "            \n",
    "            # Save the loss every `running_loss_steps` batches\n",
    "            running_loss += loss.item()\n",
    "            save_loss_condition = batch_nb % nb_steps_loss_sum == (nb_steps_loss_sum - 1)\n",
    "            if save_loss_condition:    \n",
    "                loss_list.append(running_loss)\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "            batch_nb += 1\n",
    "        \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_steps_loss_sum = 10\n",
    "batch_size = 10\n",
    "\n",
    "loss = training(batch_size=batch_size, nb_steps_loss_sum=nb_steps_loss_sum)\n",
    "\n",
    "# Plotting the loss over training\n",
    "plt.figure()\n",
    "plt.plot(range(0, len(loss)), loss)\n",
    "plt.xlabel(f\"Batches/{nb_steps_loss_sum}\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the NN accuracy\n",
    "def computeScore(X, y): \n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_size = 10\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, X.shape[0], batch_size):\n",
    "            # TO COMPLETE:\n",
    "            # - get the `batch_size` number of input samples\n",
    "            input_batch = X[idx:idx + batch_size]\n",
    "            target = y[idx:idx + batch_size]\n",
    "            \n",
    "            # - compute the prediction of the neural network\n",
    "            y_pred = my_nn(input_batch.float())\n",
    "            \n",
    "            # - get the max of the prediction (e.g. get the most likely class)\n",
    "            # This can be done using 'torch.max'\n",
    "            y_pred_max = torch.max(y_pred, 1).indices\n",
    "            \n",
    "            # - get the max of the target (e.g. correct class)\n",
    "            target_max = torch.max(target, 1).indices\n",
    "            \n",
    "            # - check if the prediction is correct and count it\n",
    "            correct += (y_pred_max == target_max).sum().item()\n",
    "            \n",
    "            # - count every sample\n",
    "            total += y_pred.size(0)   #.sum().item()\n",
    "            \n",
    "    accuracy = correct/total * 100\n",
    "    \n",
    "    print(y_pred, target)\n",
    "    print(y_pred_max, target_max)\n",
    "    print(correct, total)\n",
    "    \n",
    "    print(f\"Accuracy of the network on the {total} samples: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeScore(X_train_res, y_train_res)\n",
    "computeScore(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_project",
   "language": "python",
   "name": "pytorch_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
